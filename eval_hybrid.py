import os
import time
from datetime import datetime
import pandas as pd
from dotenv import load_dotenv

# Import all RAG pipelines
import rag_chunks_only
import rag_data_cleaned
import rag_hybrid_multiquery

from ragas import EvaluationDataset, evaluate
from ragas.llms import LangchainLLMWrapper
from ragas.metrics import ContextPrecision, Faithfulness, ResponseRelevancy
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# Load environment variables
load_dotenv()

# Advanced test queries from eval.py
advanced_sample_queries = [
    # --- è¯­ä¹‰é¸¿æ²Ÿ (ç”¨æˆ·ä¸çŸ¥é“ä¸“ä¸šæœ¯è¯­) ---
    "æ€ä¹ˆä¿è¯å‰ç«¯ä¼ ç»™æˆ‘çš„ JSON æ•°æ®é‡Œï¼Œ'price' å­—æ®µä¸€å®šæ˜¯æ•°å­—è€Œä¸æ˜¯å­—ç¬¦ä¸²ï¼Ÿå¦‚æœä¸åˆæ³•è‡ªåŠ¨æŠ¥é”™å—ï¼Ÿ",
    "æˆ‘æƒ³åœ¨å¥½å‡ ä¸ªæ¥å£é‡Œå¤ç”¨åŒä¸€æ®µä»£ç ï¼Œæ¯”å¦‚æ£€æŸ¥ç”¨æˆ· Tokenï¼Œä½†æˆ‘ä¸æƒ³å†™è£…é¥°å™¨ï¼Œä¹Ÿä¸æƒ³åœ¨æ¯ä¸ªå‡½æ•°é‡Œå†™ä¸€éè°ƒç”¨ã€‚",

    # --- é”™è¯¯å‰æ (ç”¨æˆ·é—®é¢˜åŒ…å«é”™è¯¯å‡è®¾) ---
    "å¦‚ä½•åœ¨ FastAPI çš„ settings.py é‡Œé…ç½®å†…ç½®çš„ ORM è¿æ¥æ•°æ®åº“ï¼Ÿ",
    "ä¸ºä»€ä¹ˆæˆ‘åœ¨ path operation é‡Œç”¨äº† threading.Thread å»å¼€æ–°çº¿ç¨‹å¤„ç†ä»»åŠ¡ï¼Œè¯·æ±‚è¿˜æ˜¯é˜»å¡äº†ï¼Ÿ",

    # --- å¤šè·³æ¨ç† (ç­”æ¡ˆéœ€è¦ç»¼åˆå¤šä¸ªæ¦‚å¿µ) ---
    "æˆ‘è¦åšè®¡ç®—å¯†é›†å‹ä»»åŠ¡ï¼Œåº”è¯¥ç”¨ async def è¿˜æ˜¯æ™®é€šçš„ defï¼Ÿè¿™è·Ÿ Node.js çš„å¤„ç†æ–¹å¼ä¸€æ ·å—ï¼Ÿ",
    "æˆ‘è¦è®°å½•è¯·æ±‚å¤„ç†æ—¶é—´ï¼ŒFastAPI çš„ä¸­é—´ä»¶å’Œä¾èµ–æ³¨å…¥éƒ½èƒ½åšæ‹¦æˆªï¼Œé€‰å“ªä¸ªæ›´å¥½ï¼Ÿ",

    # --- è¾¹ç¼˜æƒ…å†µ ---
    "æˆ‘æœ‰ä¸€ä¸ªå­—æ®µæ—¢å¯ä»¥æ˜¯ int ä¹Ÿå¯ä»¥æ˜¯ floatï¼Œç”šè‡³æœ‰æ—¶å€™æ˜¯ stringï¼Œæ€ä¹ˆå®šä¹‰ Schema èƒ½è®©å®ƒé€šè¿‡æ ¡éªŒï¼Ÿ",
    "æˆ‘æƒ³åœ¨æœåŠ¡å™¨å¯åŠ¨å‰é¢„åŠ è½½ä¸€ä¸ªå¾ˆå¤§çš„æœºå™¨å­¦ä¹ æ¨¡å‹åˆ°å†…å­˜é‡Œï¼ŒæœåŠ¡å…³é—­æ—¶å†é‡Šæ”¾æ‰ï¼Œä»£ç åº”è¯¥å†™åœ¨å“ªé‡Œï¼Ÿ",
    "å¦‚æœæˆ‘æƒ³åœ¨è¿™ä¸ªæ¥å£ä¸ŠåŒæ—¶ç”¨ OAuth2 éªŒè¯ï¼Œåˆè¦æ ¡éªŒä¸€ä¸ª API Key headerï¼Œæ€ä¹ˆæŠŠè¿™ä¸¤ä¸ªå®‰å…¨éªŒè¯ä¸²èµ·æ¥ï¼Ÿ",
    "Uvicorn çš„ workers æ•°é‡è®¾ç½®æˆå¤šå°‘åˆé€‚ï¼Ÿæ˜¯ä¸æ˜¯è¶Šå¤šè¶Šå¥½ï¼Ÿè·Ÿ CPU æ ¸æ•°æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ"
]

advanced_expected_responses = [
    # 1. JSON æ•°æ®æ ¡éªŒ (è¯­ä¹‰é¸¿æ²Ÿ)
    "ä½ åº”è¯¥ä½¿ç”¨ Pydantic æ¨¡å‹æ¥å®šä¹‰æ•°æ®ç»“æ„ã€‚é€šè¿‡åˆ›å»ºä¸€ä¸ªç»§æ‰¿è‡ª `pydantic.BaseModel` çš„ç±»ï¼Œå¹¶å°† 'price' å­—æ®µå£°æ˜ä¸º `float` æˆ– `int` ç±»å‹ï¼ˆä¾‹å¦‚ `price: float`ï¼‰ï¼ŒFastAPI ä¼šè‡ªåŠ¨è¿›è¡Œæ•°æ®æ ¡éªŒã€‚å¦‚æœå‰ç«¯ä¼ æ¥çš„æ•°æ®ç±»å‹ä¸åŒ¹é…ï¼ˆä¸”æ— æ³•è½¬æ¢ï¼‰ï¼ŒFastAPI ä¼šè‡ªåŠ¨æ‹¦æˆªè¯·æ±‚å¹¶è¿”å› 422 Unprocessable Entity é”™è¯¯ï¼Œå…¶ä¸­åŒ…å«è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œæ— éœ€æ‰‹åŠ¨ç¼–å†™æŠ¥é”™é€»è¾‘ã€‚",

    # 2. ä»£ç å¤ç”¨/Tokenæ£€æŸ¥ (è¯­ä¹‰é¸¿æ²Ÿ)
    "ä½ åº”è¯¥ä½¿ç”¨ FastAPI çš„ä¾èµ–æ³¨å…¥ç³»ç»Ÿï¼ˆDependency Injectionï¼‰ã€‚ä½ å¯ä»¥å®šä¹‰ä¸€ä¸ªæ™®é€šçš„å‡½æ•°ï¼ˆä¾‹å¦‚ `get_current_user`ï¼‰æ¥åŒ…å« Token æ£€æŸ¥é€»è¾‘ï¼Œç„¶ååœ¨éœ€è¦çš„è·¯ç”±å‡½æ•°å‚æ•°ä¸­ä½¿ç”¨ `Depends(get_current_user)`ã€‚è¿™æ¯”è£…é¥°å™¨æ›´çµæ´»ï¼Œå› ä¸ºä¾èµ–é¡¹å¯ä»¥åƒæ™®é€šå‡½æ•°ä¸€æ ·æ¥æ”¶å‚æ•°ï¼Œå¹¶ä¸”å¯ä»¥è¢« FastAPI è‡ªåŠ¨å¤„ç†å’Œæµ‹è¯•ï¼ŒåŒæ—¶ä¹Ÿæ”¯æŒä¾èµ–é¡¹çš„åµŒå¥—ã€‚",

    # 3. Settings/ORM é…ç½® (é”™è¯¯å‰æ)
    "è¿™æ˜¯ä¸€ä¸ªè¯¯è§£ã€‚FastAPI æ˜¯ä¸€ä¸ªæ— ä¸»è§ï¼ˆUnopinionatedï¼‰çš„æ¡†æ¶ï¼Œå®ƒæ²¡æœ‰å†…ç½®çš„ ORMï¼ˆå¯¹è±¡å…³ç³»æ˜ å°„ï¼‰æˆ–ç±»ä¼¼ Django çš„ `settings.py` é…ç½®æ–‡ä»¶ã€‚ä½ å¯ä»¥è‡ªç”±é€‰æ‹©ä»»ä½•æ•°æ®åº“åº“ï¼ˆå¦‚ SQLAlchemy, SQLModel, Tortoise ORM ç­‰ï¼‰ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨ Pydantic çš„ `BaseSettings` æ¥ç®¡ç†ç¯å¢ƒå˜é‡å’Œé…ç½®ï¼Œå¹¶åœ¨å•ç‹¬çš„æ–‡ä»¶ï¼ˆå¦‚ `database.py`ï¼‰ä¸­æ‰‹åŠ¨åˆå§‹åŒ–æ•°æ®åº“è¿æ¥ã€‚",

    # 4. Threading é˜»å¡ (é”™è¯¯å‰æ)
    "å¦‚æœåœ¨ `async def` å®šä¹‰çš„è·¯ç”±ä¸­æ‰§è¡Œè€—æ—¶æ“ä½œï¼ˆå³ä½¿æ˜¯å¯åŠ¨çº¿ç¨‹çš„å¼€é”€ï¼‰ï¼Œä»å¯èƒ½é˜»å¡ä¸»äº‹ä»¶å¾ªç¯ï¼ˆEvent Loopï¼‰ã€‚FastAPI çš„ `async def` è·¯ç”±è¿è¡Œåœ¨å•çº¿ç¨‹å¾ªç¯ä¸­ã€‚å¦‚æœä½ éœ€è¦è¿›è¡Œé˜»å¡æ€§æ“ä½œæˆ–åˆ©ç”¨å¤šæ ¸ CPUï¼Œå»ºè®®ä¸è¦æ‰‹åŠ¨ç®¡ç† `threading.Thread`ï¼Œè€Œæ˜¯å°†è·¯ç”±å®šä¹‰ä¸ºæ™®é€šçš„ `def`ï¼ˆFastAPI ä¼šè‡ªåŠ¨å°†å…¶æ”¾å…¥å¤–éƒ¨çº¿ç¨‹æ± è¿è¡Œï¼‰ï¼Œæˆ–è€…ä½¿ç”¨ `fastapi.concurrency.run_in_threadpool` æ˜¾å¼å°†ä»»åŠ¡æ´¾å‘åˆ°çº¿ç¨‹æ± ã€‚",

    # 5. è®¡ç®—å¯†é›†å‹ä»»åŠ¡é€‰æ‹© (å¤šè·³æ¨ç†)
    "å¯¹äºè®¡ç®—å¯†é›†å‹ï¼ˆCPU-boundï¼‰ä»»åŠ¡ï¼Œä½ åº”è¯¥ä½¿ç”¨æ™®é€šçš„ `def` å®šä¹‰è·¯ç”±ï¼Œæˆ–è€…ä½¿ç”¨ `run_in_process`ã€‚å› ä¸º `async def` è¿è¡Œåœ¨ä¸»äº‹ä»¶å¾ªç¯ä¸Šï¼Œè®¡ç®—å¯†é›†å‹ä»»åŠ¡ä¼šé•¿æ—¶é—´å ç”¨ CPUï¼Œå¯¼è‡´æ•´ä¸ªæœåŠ¡æ— æ³•å“åº”å…¶ä»–è¯·æ±‚ï¼ˆé˜»å¡ Loopï¼‰ã€‚è¿™ä¸ Node.js ç±»ä¼¼ï¼ŒNode.js ä¹Ÿæ˜¯å•çº¿ç¨‹äº‹ä»¶å¾ªç¯ï¼Œå¦‚æœè¿›è¡Œç¹é‡è®¡ç®—ä¹Ÿä¼šé˜»å¡æ•´ä¸ªè¿›ç¨‹ï¼Œé€šå¸¸éœ€è¦ Worker Threads æ¥è§£å†³ã€‚ä½†åœ¨ FastAPI ä¸­ï¼Œç®€å•åœ°ä½¿ç”¨ `def` å°±èƒ½åˆ©ç”¨çº¿ç¨‹æ± ï¼Œè¿™é€šå¸¸è¶³ä»¥åº”å¯¹éæç«¯çš„è®¡ç®—éœ€æ±‚ã€‚",

    # 6. ä¸­é—´ä»¶ vs ä¾èµ–æ³¨å…¥ (å¤šè·³æ¨ç†)
    "å¦‚æœç›®çš„æ˜¯è®°å½•æ•´ä¸ªè¯·æ±‚çš„å¤„ç†æ—¶é—´ï¼ˆåŒ…æ‹¬åºåˆ—åŒ–ã€éªŒè¯å’Œç½‘ç»œä¼ è¾“ï¼‰ï¼Œåº”è¯¥é€‰æ‹©ä¸­é—´ä»¶ï¼ˆMiddlewareï¼‰ï¼ˆå¦‚ `BaseHTTPMiddleware`ï¼‰ã€‚å› ä¸ºä¸­é—´ä»¶ä½œç”¨äºè¯·æ±‚-å“åº”çš„æ•´ä¸ªç”Ÿå‘½å‘¨æœŸï¼Œèƒ½å¤Ÿæ•è·ä»è¯·æ±‚åˆ°è¾¾æœåŠ¡å™¨åˆ°å“åº”å‘é€å›å®¢æˆ·ç«¯çš„å®Œæ•´è€—æ—¶ã€‚è€Œä¾èµ–æ³¨å…¥ï¼ˆDependenciesï¼‰é€šå¸¸åœ¨è·¯ç”±åŒ¹é…å’Œéƒ¨åˆ†éªŒè¯ä¹‹åæ‰æ‰§è¡Œï¼Œä¸”æ— æ³•è½»æ˜“æ•è·å“åº”å‘é€åçš„æ—¶é—´ç‚¹ã€‚",

    # 7. å¤šç§ç±»å‹å­—æ®µ (è¾¹ç¼˜æƒ…å†µ)
    "ä½ å¯ä»¥ä½¿ç”¨ Python `typing` æ¨¡å—ä¸­çš„ `Union` ç±»å‹ã€‚åœ¨ Pydantic æ¨¡å‹ä¸­ï¼Œå°†å­—æ®µå®šä¹‰ä¸º `Union[int, float, str]`ã€‚Pydantic ä¼šæŒ‰ç…§å®šä¹‰çš„é¡ºåºå°è¯•è¿›è¡Œç±»å‹æ ¡éªŒå’Œè½¬æ¢ã€‚å¦‚æœä¸€å®šè¦æ¥æ”¶ä»»æ„ç±»å‹ï¼Œå¯ä»¥ä½¿ç”¨ `Any`ï¼Œä½†è¿™ä¼šå¤±å»æ ¡éªŒçš„æ„ä¹‰ã€‚å¯¹äºå¤æ‚çš„æ¡ä»¶æ ¡éªŒï¼Œè¿˜å¯ä»¥ç»“åˆ Pydantic çš„ `validator` è£…é¥°å™¨è¿›è¡Œè‡ªå®šä¹‰é€»è¾‘åˆ¤æ–­ã€‚",

    # 8. ç”Ÿå‘½å‘¨æœŸ/é¢„åŠ è½½ (è¾¹ç¼˜æƒ…å†µ)
    "ä»£ç åº”è¯¥å†™åœ¨ Lifespanï¼ˆç”Ÿå‘½å‘¨æœŸï¼‰äº‹ä»¶å¤„ç†å™¨ä¸­ã€‚FastAPI æ¨èä½¿ç”¨ `contextlib.asynccontextmanager` è£…é¥°å™¨åˆ›å»ºä¸€ä¸ªå¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™ `FastAPI(lifespan=...)` å‚æ•°ã€‚åœ¨è¯¥å‡½æ•° `yield` å…³é”®å­—ä¹‹å‰çš„ä»£ç ä¼šåœ¨åº”ç”¨å¯åŠ¨ï¼ˆStartupï¼‰æ—¶æ‰§è¡Œï¼ˆé€‚åˆåŠ è½½æ¨¡å‹ï¼‰ï¼Œ`yield` ä¹‹åçš„ä»£ç ä¼šåœ¨åº”ç”¨å…³é—­ï¼ˆShutdownï¼‰æ—¶æ‰§è¡Œï¼ˆé€‚åˆé‡Šæ”¾å†…å­˜ï¼‰ã€‚ä¸å»ºè®®å†ä½¿ç”¨å·²åºŸå¼ƒçš„ `@app.on_event`ã€‚",

    # 9. ç»„åˆ OAuth2 å’Œ API Key (è¾¹ç¼˜æƒ…å†µ)
    "ä½ å¯ä»¥åœ¨è·¯ç”±æ“ä½œå‡½æ•°ï¼ˆPath Operationï¼‰ä¸­åŒæ—¶å£°æ˜å¤šä¸ªä¾èµ–é¡¹ã€‚å¯ä»¥å°†å®ƒä»¬ä½œä¸ºå‚æ•°åˆ†åˆ«æ³¨å…¥ï¼Œä¾‹å¦‚ `def route(token: str = Depends(oauth2_scheme), key: str = Depends(api_key_header))`ã€‚å¦‚æœä¸éœ€è¦åœ¨å‡½æ•°å†…éƒ¨ä½¿ç”¨è¿™äº›è¿”å›å€¼ï¼Œä¹Ÿå¯ä»¥å°†å®ƒä»¬æ”¾å…¥è·¯ç”±è£…é¥°å™¨çš„ `dependencies` å‚æ•°åˆ—è¡¨ä¸­ï¼Œä¾‹å¦‚ `@app.get('/items', dependencies=[Depends(oauth2_scheme), Depends(verify_api_key)])`ï¼Œè¿™æ · FastAPI ä¼šæŒ‰é¡ºåºæ‰§è¡Œæ‰€æœ‰å®‰å…¨éªŒè¯ã€‚",

    # 10. Uvicorn Workers è®¾ç½® (è¾¹ç¼˜æƒ…å†µ)
    "Uvicorn çš„ workers æ•°é‡å¹¶éè¶Šå¤šè¶Šå¥½ã€‚å®˜æ–¹å»ºè®®çš„ç»éªŒå€¼é€šå¸¸æ˜¯ CPU æ ¸å¿ƒæ•°ï¼ˆnum_coresï¼‰æˆ– CPU æ ¸å¿ƒæ•° + 1ã€‚å› ä¸º Python çš„å…¨å±€è§£é‡Šå™¨é”ï¼ˆGILï¼‰é™åˆ¶äº†å•ä¸ªè¿›ç¨‹åªèƒ½åˆ©ç”¨ä¸€ä¸ª CPU æ ¸å¿ƒï¼Œè¿‡å¤šçš„ Worker åªä¼šå¢åŠ ä¸Šä¸‹æ–‡åˆ‡æ¢çš„å¼€é”€å¹¶æ¶ˆè€—æ›´å¤šå†…å­˜ã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œé€šå¸¸é…åˆ Gunicorn ä½œä¸ºè¿›ç¨‹ç®¡ç†å™¨ï¼Œé€šè¿‡ `-w` å‚æ•°è®¾ç½® worker æ•°é‡ï¼Œå¹¶æŒ‡å®š `-k uvicorn.workers.UvicornWorker` ç±»ã€‚"
]


def evaluate_pipeline(pipeline_name, rag_chain=None, retriever=None, hybrid_components=None):
    """
    Evaluate a single RAG pipeline

    Args:
        pipeline_name: Name of the pipeline (for logging)
        rag_chain: RAG chain (for non-hybrid pipelines)
        retriever: Retriever (for non-hybrid pipelines)
        hybrid_components: Tuple of (index, embeddings, bm25, llm) for hybrid pipeline

    Returns:
        List of evaluation data dictionaries
    """
    print(f"\n{'='*60}")
    print(f"Evaluating: {pipeline_name}")
    print('='*60)

    dataset = []
    total_time = 0

    for i, (query, reference) in enumerate(zip(advanced_sample_queries, advanced_expected_responses)):
        print(f"\n[{i+1}/{len(advanced_sample_queries)}] Query: {query[:50]}...")

        start_time = time.time()

        try:
            if hybrid_components:
                # Hybrid pipeline
                index, embeddings, bm25, llm = hybrid_components

                # Get retrieved contexts
                docs = rag_hybrid_multiquery.multi_query_hybrid_search(
                    query, index, embeddings, bm25, llm, top_k=4, alpha=0.5, num_variations=3
                )
                retrieved_contexts = [doc['metadata']['text'] for doc in docs]

                # Generate response
                context = "\n\n".join(retrieved_contexts)
                prompt_text = f"""ä½ æ˜¯ä¸€ä¸ªä¸“é—¨è´Ÿè´£é—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚è¯·ç»“åˆä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å†…å®¹æ¥å›ç­”é—®é¢˜ã€‚å¦‚æœä½ æ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­å¾—åˆ°ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´æ˜ä½ ä¸çŸ¥é“ï¼Œä¸è¦å°è¯•ç¼–é€ ã€‚å›ç­”å­—æ•°è¯·æ§åˆ¶åœ¨ä¸‰å¥è¯ä»¥å†…ï¼Œå¹¶ä¿æŒè¨€ç®€æ„èµ…ã€‚
é—®é¢˜ï¼š {query}
ä¸Šä¸‹æ–‡ï¼š {context}
ç­”æ¡ˆï¼š
"""
                response = llm.invoke(prompt_text)
                response_text = response.content

            else:
                # Standard LangChain pipeline
                relevant_docs = retriever.invoke(query)
                retrieved_contexts = [doc.page_content for doc in relevant_docs]
                response_text = rag_chain.invoke(query)

            elapsed_time = time.time() - start_time
            total_time += elapsed_time

            dataset.append({
                "user_input": query,
                "retrieved_contexts": retrieved_contexts,
                "response": response_text,
                "reference": reference,
                "pipeline": pipeline_name,
                "latency_seconds": round(elapsed_time, 2)
            })

            print(f"  âœ“ Completed in {elapsed_time:.2f}s")

        except Exception as e:
            print(f"  âœ— Error: {e}")
            dataset.append({
                "user_input": query,
                "retrieved_contexts": [],
                "response": f"ERROR: {str(e)}",
                "reference": reference,
                "pipeline": pipeline_name,
                "latency_seconds": 0
            })

    avg_latency = total_time / len(advanced_sample_queries)
    print(f"\n{pipeline_name} - Average latency: {avg_latency:.2f}s")

    return dataset


def main():
    print("="*60)
    print("Comparative RAG Pipeline Evaluation")
    print("="*60)
    print("\nComparing 3 pipelines:")
    print("  1. chunks_only - Direct chunk retrieval")
    print("  2. data_cleaned - Parent-child with MultiVectorRetriever")
    print("  3. hybrid_multiquery - Hybrid search + Multi-query")
    print("\nMetrics: ContextPrecision, Faithfulness, ResponseRelevancy")

    # Initialize all pipelines
    print("\n" + "="*60)
    print("INITIALIZATION PHASE")
    print("="*60)

    print("\n[1/3] Loading chunks_only pipeline...")
    chunks_chain, chunks_retriever = rag_chunks_only.get_rag_chain()

    print("\n[2/3] Loading data_cleaned pipeline...")
    cleaned_chain, cleaned_retriever = rag_data_cleaned.get_rag_chain()

    print("\n[3/3] Loading hybrid_multiquery pipeline...")
    hybrid_index, hybrid_embeddings, hybrid_bm25, hybrid_llm = rag_hybrid_multiquery.get_rag_components()

    # Evaluate each pipeline
    print("\n" + "="*60)
    print("EVALUATION PHASE")
    print("="*60)

    all_data = []

    # Evaluate chunks_only
    chunks_data = evaluate_pipeline(
        "chunks_only",
        rag_chain=chunks_chain,
        retriever=chunks_retriever
    )
    print(f"\nchunks_data has {len(chunks_data)} entries")
    all_data.extend(chunks_data)

    # Evaluate data_cleaned
    cleaned_data = evaluate_pipeline(
        "data_cleaned",
        rag_chain=cleaned_chain,
        retriever=cleaned_retriever
    )
    print(f"\ncleaned_data has {len(cleaned_data)} entries")
    all_data.extend(cleaned_data)

    # Evaluate hybrid_multiquery
    hybrid_data = evaluate_pipeline(
        "hybrid_multiquery",
        hybrid_components=(hybrid_index, hybrid_embeddings, hybrid_bm25, hybrid_llm)
    )
    print(f"\nhybrid_data has {len(hybrid_data)} entries")
    all_data.extend(hybrid_data)

    print(f"\nTotal all_data before RAGAS: {len(all_data)} entries")

    # Run RAGAS evaluation
    print("\n" + "="*60)
    print("RAGAS METRICS CALCULATION")
    print("="*60)

    # Separate metadata from evaluation data
    # Add unique row_id to prevent Cartesian product during merge
    metadata_fields = []
    eval_data = []
    for i, item in enumerate(all_data):
        metadata_fields.append({
            'row_id': i,
            'pipeline': item['pipeline'],
            'latency_seconds': item['latency_seconds']
        })
        eval_data.append({
            'row_id': i,
            'user_input': item['user_input'],
            'retrieved_contexts': item['retrieved_contexts'],
            'response': item['response'],
            'reference': item['reference']
        })

    eval_dataset = EvaluationDataset.from_list(eval_data)

    # Initialize evaluator LLM and embeddings
    llm = ChatOpenAI(
        model="google/gemini-3-flash-preview",
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        base_url="https://openrouter.ai/api/v1",
        temperature=0
    )

    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-large",
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        base_url="https://openrouter.ai/api/v1"
    )

    evaluator_llm = LangchainLLMWrapper(llm)

    print("\nCalculating RAGAS metrics (this may take a few minutes)...")
    result = evaluate(
        dataset=eval_dataset,
        metrics=[ContextPrecision(), Faithfulness(), ResponseRelevancy()],
        llm=evaluator_llm,
        embeddings=embeddings
    )

    # Convert to DataFrame
    df = result.to_pandas()
    print(f"\nRAGAS result DataFrame has {len(df)} rows")

    # Add metadata back to DataFrame
    # RAGAS preserves order, so we can add columns directly
    metadata_df = pd.DataFrame(metadata_fields)
    print(f"Metadata DataFrame has {len(metadata_df)} rows")

    # Verify same length
    if len(df) != len(metadata_df):
        print(f"WARNING: Length mismatch! df={len(df)}, metadata={len(metadata_df)}")

    # Add metadata columns directly (relies on order preservation)
    df['pipeline'] = metadata_df['pipeline'].values
    df['latency_seconds'] = metadata_df['latency_seconds'].values

    print(f"Final DataFrame has {len(df)} rows")

    # Add summary statistics
    print("\n" + "="*60)
    print("RESULTS SUMMARY")
    print("="*60)

    # Print available columns for debugging
    print("\nAvailable columns:", df.columns.tolist())

    # Find the metric columns (they might have different names)
    metric_cols = [col for col in df.columns if col not in ['user_input', 'retrieved_contexts', 'response', 'reference', 'pipeline', 'latency_seconds', 'row_id']]
    print(f"Metric columns: {metric_cols}")

    # Build aggregation dict dynamically
    agg_dict = {'latency_seconds': 'mean'}
    for col in metric_cols:
        agg_dict[col] = 'mean'

    summary = df.groupby('pipeline').agg(agg_dict).round(4)

    print("\n" + summary.to_string())

    # Export detailed results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    detail_filename = f"eval_comparison_detailed_{timestamp}.csv"
    summary_filename = f"eval_comparison_summary_{timestamp}.csv"

    df.to_csv(detail_filename, index=False, encoding='utf-8-sig')
    summary.to_csv(summary_filename, encoding='utf-8-sig')

    print(f"\nâœ“ Detailed results exported to: {detail_filename}")
    print(f"âœ“ Summary results exported to: {summary_filename}")

    # Identify winner
    print("\n" + "="*60)
    print("WINNER ANALYSIS")
    print("="*60)

    # Print best pipeline for each metric
    print("\n")
    for col in metric_cols:
        best_pipeline = summary[col].idxmax()
        best_score = summary.loc[best_pipeline, col]
        print(f"ğŸ† Best {col}: {best_pipeline} ({best_score:.4f})")

    fastest = summary['latency_seconds'].idxmin()
    print(f"âš¡ Fastest: {fastest} ({summary.loc[fastest, 'latency_seconds']:.2f}s)")

    print("\n" + "="*60)
    print("Evaluation complete!")
    print("="*60)


if __name__ == "__main__":
    main()
